{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import random\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "target = iris.target\n",
    "data = pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "data.columns = ['sepal_length','sepal_width','petal_length','petal_width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example = data.copy()\n",
    "Example['Label'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_train['label'] = y_train\n",
    "train_df = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_test['label'] = y_test\n",
    "test_df = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_purity(data):\n",
    "    \n",
    "    '''\n",
    "   Returns a count of the unique classes in a leaf node.\n",
    "   \n",
    "    '''\n",
    "    \n",
    "    label = data[:,-1]\n",
    "    #returns array of our unique labels\n",
    "    unique_classes = np.unique(label)\n",
    "    \n",
    "    if len(unique_classes)==1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_purity(train_df[train_df['petal_length']>7].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(data):\n",
    "    \n",
    "    '''\n",
    "    Counts the number of samples related to each classification \n",
    "    and returns that class that has the majority vote\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    label = data[:,-1]\n",
    "    unique_classes, counts_of_unique_classes = np.unique(label,return_counts=True)\n",
    "\n",
    "    index = counts_of_unique_classes.argmax()\n",
    "\n",
    "    classification = unique_classes[index]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_splits(data,column_list):\n",
    "    \n",
    "    '''\n",
    "    Iterates over each column and returns the unqiue values\n",
    "    as potential decision split nodes\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "   \n",
    "\n",
    "    for column_index in range(n_columns-1):\n",
    "        \n",
    "        values = data[:, column_index]\n",
    "        univalues = np.unique(values)\n",
    "        \n",
    "        potential_splits[column_list[column_index]] = univalues\n",
    "        \n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'petal_length': array([1. , 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.9, 3. , 3.3, 3.5, 3.6, 3.7,\n",
      "       3.8, 3.9, 4. , 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5. ,\n",
      "       5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6. , 6.1, 6.3, 6.6,\n",
      "       6.7, 6.9]),\n",
      " 'petal_width': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 1. , 1.1, 1.2, 1.3, 1.4, 1.5, 1.6,\n",
      "       1.7, 1.8, 1.9, 2. , 2.1, 2.3, 2.4, 2.5]),\n",
      " 'sepal_length': array([4.5, 4.6, 4.7, 4.8, 4.9, 5. , 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7,\n",
      "       5.8, 5.9, 6. , 6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8, 6.9, 7. ,\n",
      "       7.1, 7.2, 7.3, 7.6, 7.7]),\n",
      " 'sepal_width': array([2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2, 3.3, 3.4,\n",
      "       3.5, 3.6, 3.7, 3.8, 4. , 4.1, 4.4])}\n"
     ]
    }
   ],
   "source": [
    "potential_splits = get_potential_splits(train_df.values,train_df.columns)\n",
    "pprint(potential_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    '''\n",
    "    This function will perform the optimal split given the scoring metric (entropy/mse).\n",
    "    If the feature is a continuous variable, the function will split based on values\n",
    "    that fall above or below the decision node. \n",
    "    \n",
    "    For categorical, the function will split using equal to and is not equal to logic.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    split_column_values = data[:, split_column]\n",
    "    \n",
    "    type_of_feature = FEATURE_TYPES[split_column]\n",
    "    if type_of_feature == 'continuous':\n",
    "    \n",
    "        data_below = data[split_column_values <=split_value]\n",
    "        data_above = data[split_column_values >split_value]\n",
    "\n",
    "    else:\n",
    "        data_below = data[split_column_values ==split_value]\n",
    "        data_above = data[split_column_values !=split_value]\n",
    "        \n",
    "    \n",
    "    return data_below, data_above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowest Overall Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(data):\n",
    "    label_column = data[:,-1]\n",
    "    ##determine probabilities of the classes\n",
    "\n",
    "    #count up the number of samples in each label\n",
    "    _,counts = np.unique(label_column,return_counts=True)\n",
    "    \n",
    "    #convert the count to the probability of a value falling into a certain class label\n",
    "    probabilities = counts/counts.sum()\n",
    "\n",
    "    \n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_entropy(data_below,data_above):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Combines the entropy metric of values that fall above and below the decision split\n",
    "    and returns the score\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    n_data_points = len(data_below) + len(data_above)\n",
    "\n",
    "\n",
    "    #samples below the line\n",
    "    p_data_below = len(data_below)/n_data_points\n",
    "\n",
    "    #samples above the line\n",
    "    p_data_above = len(data_above)/n_data_points\n",
    "\n",
    "    overall_entropy = (p_data_below * calculate_entropy(data_below)\n",
    "\n",
    "                      +p_data_above * calculate_entropy(data_above))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return overall_entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the splits that results in the lowest overall entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_split(data, potential_splits):\n",
    "    \n",
    "    overall_entropy = 999\n",
    "\n",
    "    \"\"\"\n",
    "    We start off by setting the initial overall entropy arbitrarily high.\n",
    "    \n",
    "    The function then Iterates over each split list and calculates the overall entropy of the current\n",
    "    decision split. If new split value results in a lower entropy score, the funciton will replace the\n",
    "    old entropy metric. \n",
    "    \"\"\"\n",
    "\n",
    "    for column_index in potential_splits:\n",
    "        \n",
    "        \n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data,split_column=column_index,split_value=value)\n",
    "\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below,data_above)\n",
    "\n",
    "            if current_overall_entropy <= overall_entropy:\n",
    "\n",
    "                overall_entropy=current_overall_entropy\n",
    "\n",
    "\n",
    "                best_split_column = column_index\n",
    "\n",
    "                best_split_value = value\n",
    "                \n",
    "                \n",
    "    return best_split_column,best_split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sepal_length': array([4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5. , 5.1, 5.2, 5.3, 5.4, 5.5,\n",
       "        5.6, 5.7, 5.8, 5.9, 6. , 6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8,\n",
       "        6.9, 7. , 7.1, 7.2, 7.3, 7.4, 7.6, 7.7, 7.9]),\n",
       " 'sepal_width': array([2. , 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2, 3.3,\n",
       "        3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4. , 4.1, 4.2, 4.4]),\n",
       " 'petal_length': array([1. , 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.9, 3. , 3.3, 3.5, 3.6,\n",
       "        3.7, 3.8, 3.9, 4. , 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9,\n",
       "        5. , 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6. , 6.1, 6.3,\n",
       "        6.4, 6.6, 6.7, 6.9]),\n",
       " 'petal_width': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 1. , 1.1, 1.2, 1.3, 1.4, 1.5, 1.6,\n",
       "        1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_splits = get_potential_splits(Example.values,Example.columns)\n",
    "potential_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a34c13caf9d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetermine_best_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpotential_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-7c62775d7d5e>\u001b[0m in \u001b[0;36mdetermine_best_split\u001b[0;34m(data, potential_splits)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpotential_splits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mdata_below\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_above\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mcurrent_overall_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_overall_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_below\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_above\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-625fa0e52860>\u001b[0m in \u001b[0;36msplit_data\u001b[0;34m(data, split_column, split_value)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msplit_column_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtype_of_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFEATURE_TYPES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "determine_best_split(Example.values,potential_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_type_of_feature(data):\n",
    "    feature_types = []\n",
    "    n_unique_values_threshold = 15\n",
    "    for column in data.columns:\n",
    "        unique_vals = data[column].unique()\n",
    "        example_value = unique_vals[0]\n",
    "        \n",
    "        if type(example_value) == str or (len(unique_vals)<= n_unique_values_threshold):\n",
    "            feature_types.append(\"categorical\")\n",
    "        else:\n",
    "            feature_types.append(\"continuous\")\n",
    "    return feature_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sub_tree = {question: [yes_answer, no_answer]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreeAlgo(df, counter=0, min_samples=2, max_depth=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    minimum sample size: the minimum number of sampels a node must contain in\n",
    "    order to consider splitting.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # data preparations\n",
    "    \n",
    "    if counter == 0:\n",
    "        \n",
    "        global COLUMN_HEADERS, FEATURE_TYPES\n",
    "        \n",
    "        \n",
    "        COLUMN_HEADERS = df.columns\n",
    "        FEATURE_TYPES = determine_type_of_feature(df)\n",
    "        \n",
    "        data = df.values\n",
    "    else:\n",
    "        \n",
    "        data = df\n",
    "        \n",
    "        \n",
    "    # base case for recusive function\n",
    "    if (check_purity(data)) or (len(data)< min_samples) or (counter==max_depth):\n",
    "        #return our prediction\n",
    "        classification = classify_data(data)\n",
    "        return classification\n",
    "    \n",
    "    \n",
    "    #recursive section\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        \n",
    "        # run helper functions\n",
    "        \n",
    "        potential_splits = get_potential_splits(data)\n",
    "  \n",
    "            \n",
    "        #find lowest overall entropy\n",
    "        \n",
    "        split_column, split_value = determine_best_split(data, potential_splits)\n",
    "        \n",
    "        \n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        \n",
    "        \n",
    "        #check for empty data\n",
    "        if len(data_below) == 0 or len(data_above)==0:\n",
    "            classification = classify_data(data)\n",
    "            return classification\n",
    "        \n",
    "        \n",
    "        #instantiate sub-tree\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        \n",
    "        type_of_feature = FEATURE_TYPES[split_column]\n",
    "        if type_of_feature == 'continuous':\n",
    "            question  = \"{} <= {}\".format(feature_name,split_value)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            question  = \"{} == {}\".format(feature_name,split_value)\n",
    "\n",
    "        \n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        \n",
    "        #find answers\n",
    "        \n",
    "        yes_answer = DecisionTreeAlgo(data_below, counter, min_samples,max_depth)\n",
    "        \n",
    "        \n",
    "        no_answer = DecisionTreeAlgo(data_above, counter, min_samples,max_depth)\n",
    "        \n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            \n",
    "        \n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        \n",
    "        return sub_tree\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeAlgo(train_df,min_samples=5,max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "\n",
    "    feature_name, comparison, value = question.split()\n",
    "\n",
    "    # ask question\n",
    "    \n",
    "    if comparison == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0] \n",
    "        else:\n",
    "            answer = tree[question][1] \n",
    "            \n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0] \n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "        \n",
    "        \n",
    "\n",
    "    #base case    \n",
    "    if not isinstance(answer, dict):\n",
    "        return(answer)\n",
    "    else:\n",
    "\n",
    "        residual_tree = answer\n",
    "        return predict(example,residual_tree) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df,tree):\n",
    "    \n",
    "    df['classification'] = df.apply(predict, axis=1, args=(tree,))\n",
    "    \n",
    "    df['classification_correct'] = df.classification == df.label\n",
    "    \n",
    "    \n",
    "    accuracy = df.classification_correct.mean()\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(test_df,tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "split_column2 = 3\n",
    "split_value2 = 1.75\n",
    "split_value = 0.8\n",
    "split_value3 = 5.35\n",
    "#data_below, data_above = split_data(Example.values,split_column,split_value)\n",
    "data_belowsplit2, data_abovesplit2 = split_data(train_df.values,split_column2,split_value2)\n",
    "\n",
    "\n",
    "\n",
    "plotting_df = pd.DataFrame(data_belowsplit2,columns=train_df.columns)\n",
    "\n",
    "sns.lmplot(data=train_df,x='petal_width',y='petal_length',fit_reg=False,size=6,aspect=1.5,\n",
    "          hue='label')\n",
    "plt.vlines(x=split_value2,ymin=1,ymax=split_value3)\n",
    "\n",
    "plt.vlines(x=split_value,ymin=1,ymax=7)\n",
    "plt.hlines(y=split_value3,xmin=split_value,xmax=split_value2)\n",
    "\n",
    "plt.title('Petal Leaves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision With Categories Tree Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "tree = DecisionTreeAlgo(train_df,min_samples=3,max_depth=10)\n",
    "pprint(tree, width=50,depth=10)\n",
    "\n",
    "\n",
    "calculate_accuracy(test_df,tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
